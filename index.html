<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Learning disentangled visual concepts to generate different concepts in a reference image or combine concepts from different images.">
  <meta name="keywords" content="Concept Disentangle, Diffusion, Image Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OmniPrism: Learning Disentangled Visual Concept for Image Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">OmniPrism: Learning Disentangled Visual Concept for Image Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/Tale17">Yangyang Li</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://daqingliu.github.io/">Daqing Liu</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://faculty.ustc.edu.cn/liuwu">Wu Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Allen He</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://xinchenliu.com">Xinchen Liu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=hxGs4ukAAAAJ">Yongdong Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://dblp.org/pid/61/9936">Guoqing Jin</a><sup>3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Science and Technology of China,</span>
            <span class="author-block"><sup>2</sup>JD Explore Academy, JD.com Inc.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Tale17/OmniPrism"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- 主图. -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/fig1.png"
       class="interpolation-image"
       alt="Interpolate start reference image."/>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">We</span> propose OmniPrism, which arbitrarily disentangles and combines visual concepts. 
        (a) Disentangled visual concept generation. Given a reference image with multiple concepts, 
        our method can disentangle the desired concept guided by natural language such as content names 
        (red color words in prompts), “style” or “composition” (e.g., relation or structural features 
        like pose) while remaining faithful to prompts. (b) Multi-concept combination. Given two or more 
        reference images with the corresponding concept guidance, our approach can combine all desired 
        concepts in any combination without conflicts.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Creative visual concept generation often draws inspiration from specific concepts in a reference image to produce relevant outcomes.
            However, existing methods are typically constrained to single-aspect concept generation or are easily disrupted by irrelevant concepts in multi-aspect concept scenarios, leading to concept confusion and hindering creative generation. 
            To address this, we propose OmniPrism, a visual concept disentangling approach for creative image generation.
            Our method learns disentangled concept representations guided by natural language and trains a diffusion model to incorporate these concepts.
            We utilize the rich semantic space of a multimodal extractor to achieve concept disentanglement from given images and concept guidance.
            To disentangle concepts with different semantics, we construct a paired concept disentangled dataset (PCD-200K), where each pair shares the same concept such as content, style, and composition.
            We learn disentangled concept representations through our contrastive orthogonal disentangled (COD) training pipeline, which are then injected into additional diffusion cross-attention layers for generation.
            A set of block embeddings is designed to adapt each block's concept domain in the diffusion models.
            Extensive experiments demonstrate that our method can generate high-quality, concept-disentangled results with high fidelity to text prompts and desired concepts. Our codes, models, and datasets will be available upon acceptance.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method</h2>
        <img src="./static/images/method.png"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
        
        <p>
          (a) Given the reference image <code>I<sub>ref</sub></code>, target prompt <code>T<sub>tar</sub></code> and concept guidance <code>T<sub>cg</sub></code>, the concept extractor disentangles concept representations <code>f<sub>cpt</sub></code> by concatenating CLIP features <code>f<sub>cg</sub></code> of <code>T<sub>cg</sub></code> with a learnable query <code>q</code>, and feeds <code>f<sub>cpt</sub></code> into additional cross-attention layers in U-Net to generate target image <code>I<sub>tar</sub></code>. A learnable block embedding <code>e<sub>i</sub></code> is added to <code>q</code> to align the concept domain of i-th diffusion block.
        <p>
        <p>
          (b) We employ an anti-query <code>q<sub>a</sub></code> to capture irrelevant concepts <code>f<sup>a</sup><sub>cpt</sub></code> in <code>I<sub>ref</sub></code>, and constrain the desired concept <code>f<sup>tar</sup><sub>cpt</sub></code> in <code>I<sub>tar</sub></code> to be similar to <code>f<sub>cpt</sub></code> and orthogonal to <code>f<sup>a</sup><sub>cpt</sub></code> by Contrastive Orthogonal Disentangled (COD) Learning.
        </p>
      </div>
    </div>
    <!--/ Method. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
